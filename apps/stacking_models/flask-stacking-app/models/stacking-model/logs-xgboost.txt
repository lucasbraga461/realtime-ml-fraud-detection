% python notebooks/models-to-deploy/c-train_stacking_model.py
/Users/lucasbraga/Documents/GitHub/fraud-research/venv_tf/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names
  warnings.warn(
/Users/lucasbraga/Documents/GitHub/fraud-research/venv_tf/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names
  warnings.warn(
/Users/lucasbraga/Documents/GitHub/fraud-research/venv_tf/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names
  warnings.warn(
Optimizing meta-learner with Optuna...
[I 2025-05-15 22:00:07,952] A new study created in memory with name: no-name-a8519d55-e5c5-45e9-845d-6b4bb5ffc8b7
[I 2025-05-15 22:00:08,689] Trial 0 finished with value: 0.8 and parameters: {'max_depth': 10, 'learning_rate': 0.04486225817643158, 'n_estimators': 189}. Best is trial 0 with value: 0.8.
[I 2025-05-15 22:00:08,964] Trial 1 finished with value: 0.7969924812030075 and parameters: {'max_depth': 7, 'learning_rate': 0.02691492038974076, 'n_estimators': 102}. Best is trial 0 with value: 0.8.
[I 2025-05-15 22:00:09,147] Trial 2 finished with value: 0.7971014492753623 and parameters: {'max_depth': 6, 'learning_rate': 0.1693121153228347, 'n_estimators': 71}. Best is trial 0 with value: 0.8.
[I 2025-05-15 22:00:09,281] Trial 3 finished with value: 0.7878787878787878 and parameters: {'max_depth': 3, 'learning_rate': 0.03197145460662667, 'n_estimators': 87}. Best is trial 0 with value: 0.8.
[I 2025-05-15 22:00:09,380] Trial 4 finished with value: 0.0 and parameters: {'max_depth': 7, 'learning_rate': 0.223654124536675, 'n_estimators': 78}. Best is trial 0 with value: 0.8.
[I 2025-05-15 22:00:09,789] Trial 5 finished with value: 0.803030303030303 and parameters: {'max_depth': 6, 'learning_rate': 0.01693680972265667, 'n_estimators': 176}. Best is trial 5 with value: 0.803030303030303.
[I 2025-05-15 22:00:09,908] Trial 6 finished with value: 0.803030303030303 and parameters: {'max_depth': 4, 'learning_rate': 0.03746727349980626, 'n_estimators': 62}. Best is trial 5 with value: 0.803030303030303.
[I 2025-05-15 22:00:10,470] Trial 7 finished with value: 0.803030303030303 and parameters: {'max_depth': 10, 'learning_rate': 0.020910167716570612, 'n_estimators': 170}. Best is trial 5 with value: 0.803030303030303.
[I 2025-05-15 22:00:10,751] Trial 8 finished with value: 0.8059701492537313 and parameters: {'max_depth': 10, 'learning_rate': 0.043093839346247546, 'n_estimators': 79}. Best is trial 8 with value: 0.8059701492537313.
[I 2025-05-15 22:00:10,984] Trial 9 finished with value: 0.7910447761194029 and parameters: {'max_depth': 4, 'learning_rate': 0.06898557168242102, 'n_estimators': 135}. Best is trial 8 with value: 0.8059701492537313.
[I 2025-05-15 22:00:11,409] Trial 10 finished with value: 0.7910447761194029 and parameters: {'max_depth': 9, 'learning_rate': 0.08304153764285707, 'n_estimators': 125}. Best is trial 8 with value: 0.8059701492537313.
[I 2025-05-15 22:00:11,787] Trial 11 finished with value: 0.803030303030303 and parameters: {'max_depth': 6, 'learning_rate': 0.014960906948108983, 'n_estimators': 160}. Best is trial 8 with value: 0.8059701492537313.
[I 2025-05-15 22:00:12,345] Trial 12 finished with value: 0.803030303030303 and parameters: {'max_depth': 8, 'learning_rate': 0.011061616844917299, 'n_estimators': 198}. Best is trial 8 with value: 0.8059701492537313.
[I 2025-05-15 22:00:12,603] Trial 13 finished with value: 0.7910447761194029 and parameters: {'max_depth': 5, 'learning_rate': 0.11543474760268567, 'n_estimators': 120}. Best is trial 8 with value: 0.8059701492537313.
[I 2025-05-15 22:00:13,070] Trial 14 finished with value: 0.7969924812030075 and parameters: {'max_depth': 8, 'learning_rate': 0.0196620374307043, 'n_estimators': 153}. Best is trial 8 with value: 0.8059701492537313.
[I 2025-05-15 22:00:13,385] Trial 15 finished with value: 0.8059701492537313 and parameters: {'max_depth': 8, 'learning_rate': 0.05434785359477533, 'n_estimators': 101}. Best is trial 8 with value: 0.8059701492537313.
[I 2025-05-15 22:00:13,570] Trial 16 finished with value: 0.8 and parameters: {'max_depth': 9, 'learning_rate': 0.05541208000569673, 'n_estimators': 52}. Best is trial 8 with value: 0.8059701492537313.
[I 2025-05-15 22:00:13,924] Trial 17 finished with value: 0.8059701492537313 and parameters: {'max_depth': 9, 'learning_rate': 0.10783017623753206, 'n_estimators': 103}. Best is trial 8 with value: 0.8059701492537313.
[I 2025-05-15 22:00:14,226] Trial 18 finished with value: 0.8059701492537313 and parameters: {'max_depth': 8, 'learning_rate': 0.05430920625559947, 'n_estimators': 97}. Best is trial 8 with value: 0.8059701492537313.
[I 2025-05-15 22:00:14,654] Trial 19 finished with value: 0.7969924812030075 and parameters: {'max_depth': 10, 'learning_rate': 0.09731902871437606, 'n_estimators': 117}. Best is trial 8 with value: 0.8059701492537313.
Best params: {'max_depth': 10, 'learning_rate': 0.043093839346247546, 'n_estimators': 79}

Training stacking model with optimized meta-learner:
Optimal Threshold: 0.6072
Meta-model Test F1-score: 0.7687
Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     85295
           1       0.81      0.73      0.77       148

    accuracy                           1.00     85443
   macro avg       0.91      0.86      0.88     85443
weighted avg       1.00      1.00      1.00     85443